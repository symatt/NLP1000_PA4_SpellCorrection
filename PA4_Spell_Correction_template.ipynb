{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Go, Wilfred | Sibug, Jordan | Sy, James Matthew\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/739602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\JAMES\n",
      "[nltk_data]     SY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load the words from the Gutenberg Document\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# print(\"Extracting all documents from NLTK's Project Gutenberg Collection...\")\n",
    "all_words = Counter()\n",
    "for filename in nltk.corpus.gutenberg.fileids():\n",
    "    words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
    "    all_words.update(words)\n",
    "#   print(\"%s; tokens: %d; vocab: %d\" % (filename, len(words), len(set(words))))\n",
    "\n",
    "# print(\"Overall Statistics\")\n",
    "# total_tokens = sum(all_words.values())\n",
    "total_types = len(all_words)\n",
    "\n",
    "# print(\"Total tokens: %d\" % total_tokens)\n",
    "# print(\"Total vocabulary / type: %d\" % total_types)\n",
    "# print(\"Type/token ratio: %.4f\" % (total_types / total_tokens))\n",
    "# print(\"Vocabulary richness: %.4f\" % (total_types / (total_tokens ** (1/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "soul\n",
      "destiny\n",
      "i\n",
      "near\n",
      "walt\n",
      "133583\n"
     ]
    }
   ],
   "source": [
    "# print the word\n",
    "print(words[500])\n",
    "print(words[100])\n",
    "print(words[664])\n",
    "print(words[32])\n",
    "print(words[10000])\n",
    "print(words[5])\n",
    "\n",
    "# get the count of the word\n",
    "print(all_words[words[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Noisy Channel Model Implementation**\n",
    "\n",
    "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below. And while we discussed the general approach in class, kindly describe how you decided to implement the spell correction model. Include any modifications your group made as well. This might be a good spot to place part of your write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from http://norvig.com/spell-correct.html\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    \n",
    "    wordsAndEdits = {\n",
    "        'del': deletes,\n",
    "        'tra': transposes,\n",
    "        'sub': replaces,\n",
    "        'ins': inserts\n",
    "    }\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "    return wordsAndEdits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCorrect(word):\n",
    "    # check if the word is in the corpus\n",
    "    if word in words:\n",
    "        return \"No error\"\n",
    "\n",
    "    # word should be corrected\n",
    "    # get possible words within edit distance 1\n",
    "    candidates = edits1(word)\n",
    "\n",
    "    candidatesProb = {\n",
    "        'del': [],\n",
    "        'tra': [],\n",
    "        'sub': [],\n",
    "        'ins': []\n",
    "    }\n",
    "#     print(wordsAndEdits)\n",
    "    edit_types = ['del', 'tra', 'sub', 'ins']\n",
    "    \n",
    "# find the probability of the candidate in the corpus\n",
    "    for etype in edit_types:\n",
    "        for candidate in candidates[etype]:\n",
    "            if candidate in words:\n",
    "                candidatesProb[etype].append(all_words[candidate] / total_types)\n",
    "            else:\n",
    "                candidatesProb[etype].append(0)\n",
    "\n",
    "# remove the candidates with 0 probability\n",
    "    for etype in edit_types:\n",
    "        i = 0    \n",
    "        while i != len(candidates[etype]):\n",
    "            if candidatesProb[etype][i] == 0:\n",
    "                del candidates[etype][i]\n",
    "                del candidatesProb[etype][i]\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "    print(candidates)\n",
    "    print(candidatesProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hte\n",
      "{'del': ['te', 'he', 'ht'], 'tra': ['the', 'het'], 'sub': ['ate', 'bte', 'cte', 'dte', 'ete', 'fte', 'gte', 'hte', 'ite', 'jte', 'kte', 'lte', 'mte', 'nte', 'ote', 'pte', 'qte', 'rte', 'ste', 'tte', 'ute', 'vte', 'wte', 'xte', 'yte', 'zte', 'hae', 'hbe', 'hce', 'hde', 'hee', 'hfe', 'hge', 'hhe', 'hie', 'hje', 'hke', 'hle', 'hme', 'hne', 'hoe', 'hpe', 'hqe', 'hre', 'hse', 'hte', 'hue', 'hve', 'hwe', 'hxe', 'hye', 'hze', 'hta', 'htb', 'htc', 'htd', 'hte', 'htf', 'htg', 'hth', 'hti', 'htj', 'htk', 'htl', 'htm', 'htn', 'hto', 'htp', 'htq', 'htr', 'hts', 'htt', 'htu', 'htv', 'htw', 'htx', 'hty', 'htz'], 'ins': ['ahte', 'bhte', 'chte', 'dhte', 'ehte', 'fhte', 'ghte', 'hhte', 'ihte', 'jhte', 'khte', 'lhte', 'mhte', 'nhte', 'ohte', 'phte', 'qhte', 'rhte', 'shte', 'thte', 'uhte', 'vhte', 'whte', 'xhte', 'yhte', 'zhte', 'hate', 'hbte', 'hcte', 'hdte', 'hete', 'hfte', 'hgte', 'hhte', 'hite', 'hjte', 'hkte', 'hlte', 'hmte', 'hnte', 'hote', 'hpte', 'hqte', 'hrte', 'hste', 'htte', 'hute', 'hvte', 'hwte', 'hxte', 'hyte', 'hzte', 'htae', 'htbe', 'htce', 'htde', 'htee', 'htfe', 'htge', 'hthe', 'htie', 'htje', 'htke', 'htle', 'htme', 'htne', 'htoe', 'htpe', 'htqe', 'htre', 'htse', 'htte', 'htue', 'htve', 'htwe', 'htxe', 'htye', 'htze', 'htea', 'hteb', 'htec', 'hted', 'htee', 'htef', 'hteg', 'hteh', 'htei', 'htej', 'htek', 'htel', 'htem', 'hten', 'hteo', 'htep', 'hteq', 'hter', 'htes', 'htet', 'hteu', 'htev', 'htew', 'htex', 'htey', 'htez']}\n",
      "{'del': [0, 0.610713526535818, 0], 'tra': [3.1550816032499585, 0], 'sub': [0.0009683743120999551, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0003779021705755922, 0, 0, 0, 0, 0, 0.0006140910271853374, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'ins': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.004322256075958336, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "0.610713526535818\n",
      "{'del': ['he'], 'tra': ['the'], 'sub': ['ate', 'hoe', 'hue'], 'ins': ['hate']}\n",
      "{'del': [0.610713526535818], 'tra': [3.1550816032499585], 'sub': [0.0009683743120999551, 0.0003779021705755922, 0.0006140910271853374], 'ins': [0.004322256075958336]}\n",
      "Output: None\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Input: \")\n",
    "\n",
    "# call functions, return answer\n",
    "output = spellCorrect(word)\n",
    "print(\"Output: \" + str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4_Spell_Correction_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
