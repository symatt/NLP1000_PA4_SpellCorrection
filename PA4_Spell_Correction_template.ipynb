{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Go, Wilfred | Sibug, Jordan | Sy, James Matthew\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/739602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\JAMES\n",
      "[nltk_data]     SY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "# load the words from the Gutenberg Document\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# print(\"Extracting all documents from NLTK's Project Gutenberg Collection...\")\n",
    "all_words = Counter()\n",
    "for filename in nltk.corpus.gutenberg.fileids():\n",
    "    words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
    "    all_words.update(words)\n",
    "#   print(\"%s; tokens: %d; vocab: %d\" % (filename, len(words), len(set(words))))\n",
    "\n",
    "# print(\"Overall Statistics\")\n",
    "# total_tokens = sum(all_words.values())\n",
    "total_types = len(all_words)\n",
    "\n",
    "# print(\"Total tokens: %d\" % total_tokens)\n",
    "# print(\"Total vocabulary / type: %d\" % total_types)\n",
    "# print(\"Type/token ratio: %.4f\" % (total_types / total_tokens))\n",
    "# print(\"Vocabulary richness: %.4f\" % (total_types / (total_tokens ** (1/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the word\n",
    "# print(set(words))\n",
    "# mother 10\n",
    "# other 5\n",
    "# the 100\n",
    "# print(words[500])\n",
    "# print(words[100])\n",
    "# print(words[664])\n",
    "# print(words[32])\n",
    "# print(words[10000])\n",
    "# print(words[5])\n",
    "\n",
    "# get the count of the word\n",
    "# print(all_words[words[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Noisy Channel Model Implementation**\n",
    "\n",
    "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below. And while we discussed the general approach in class, kindly describe how you decided to implement the spell correction model. Include any modifications your group made as well. This might be a good spot to place part of your write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from http://norvig.com/spell-correct.html\n",
    "def edits1(word):\n",
    "    deletes = [] \n",
    "    transposes = [] \n",
    "    replaces = [] \n",
    "    inserts = []\n",
    "    deleteEdits = [] \n",
    "    transposeEdits = []\n",
    "    replaceEdits = []\n",
    "    insertEdits = []\n",
    "    deleteCorrect = [] \n",
    "    transposeCorrect = []\n",
    "    replaceCorrect = []\n",
    "    insertCorrect = []\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz-\\'\"\n",
    "#     [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "# DELETION\n",
    "# '' + 'te' = 'te' \n",
    "# if L[-1] == '': ('>' + R[0])|('>') == >h|>\n",
    "# 'h' + 'e' = 'he'  \n",
    "# (L[-1] + R[0])|(L[-1]) == ht|h\n",
    "# 'ht' + '' = 'ht'\n",
    "# (L[-1] + R[0])|(L[-1]) = te|t\n",
    "    for L,R in splits:\n",
    "        if R:\n",
    "            deletes.append(L + R[1:])\n",
    "            if not L: \n",
    "                deleteEdits.append('>' + str(R[0]) + '|>')\n",
    "                deleteCorrect.append('')\n",
    "            else:\n",
    "                deleteEdits.append(str(L[-1] + R[0]) + '|' + str(L[-1]))\n",
    "                deleteCorrect.append(str(L[-1]))\n",
    "\n",
    "# TRANSPOSITION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# ht|th R[0]+R[1] + '|' + R[1] + R[0]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    for L, R in splits:\n",
    "        if len(R) > 1:\n",
    "            transposes.append(L + R[1] + R[0] + R[2:])\n",
    "            transposeEdits.append(R[0]+R[1] + '|' + R[1] + R[0])\n",
    "            transposeCorrect.append(R[1] + R[0])\n",
    "# SUBSTITUTION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# e|i \n",
    "# R[0] + '|' + c\n",
    "# \n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            for c in letters:\n",
    "                replaces.append(L + c + R[1:])\n",
    "                replaceEdits.append(R[0] + '|' + c)\n",
    "                replaceCorrect.append(c)\n",
    "\n",
    "# INSERTION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# \n",
    "# >|>x if not L '>|>' + c\n",
    "# h|ha str(L[-1]) + '|' + str(L[-1]) + c\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    for L, R in splits:\n",
    "        for c in letters:\n",
    "            inserts.append(L + c + R)\n",
    "            if not L:\n",
    "                insertEdits.append('>|>' + c)\n",
    "                insertCorrect.append(c)\n",
    "            else:\n",
    "                insertEdits.append(str(L[-1]) + '|' + str(L[-1]) + c)\n",
    "                insertCorrect.append(c)\n",
    "    \n",
    "    # print(deleteEdits)\n",
    "    # print(insertEdits)\n",
    "    # print(replaceEdits)\n",
    "    # print(transposeEdits)\n",
    "    \n",
    "    wordsAndEdits = {\n",
    "        'del': deletes,\n",
    "        'tra': transposes,\n",
    "        'sub': replaces,\n",
    "        'ins': inserts,\n",
    "        'delEdits': deleteEdits,\n",
    "        'traEdits': transposeEdits,\n",
    "        'subEdits': replaceEdits,\n",
    "        'insEdits': insertEdits,\n",
    "        'delCorrect': deleteCorrect,\n",
    "        'traCorrect': transposeCorrect,\n",
    "        'subCorrect': replaceCorrect,\n",
    "        'insCorrect': insertCorrect\n",
    "    }\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "    return wordsAndEdits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_err_model():\n",
    "    err_model = {}\n",
    "    err_count = 0\n",
    "\n",
    "    count_1edit = open('count_1edit.txt', 'r')\n",
    "\n",
    "    for line in count_1edit:\n",
    "        # ht|th -> key.split('|') -> x[1]  \n",
    "        content = line.rstrip().split(\"\\t\")\n",
    "        key = str(content[0])\n",
    "        val = float(content[1])\n",
    "        err_model[key] = val\n",
    "\n",
    "    # print(err_model)\n",
    "\n",
    "    count_1edit.close()\n",
    "\n",
    "    return err_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCorrect(word):\n",
    "\n",
    "    err_model = load_err_model()\n",
    "    temp = []\n",
    "    # check if the word is in the corpus\n",
    "    if word in words:\n",
    "        return temp\n",
    "\n",
    "    # word should be corrected\n",
    "    # get possible words within edit distance 1\n",
    "    candidates = edits1(word)\n",
    "\n",
    "    candidatesProb = {\n",
    "        'del': [],\n",
    "        'tra': [],\n",
    "        'sub': [],\n",
    "        'ins': []\n",
    "    }\n",
    "#     print(wordsAndEdits)\n",
    "    edit_types = ['del', 'tra', 'sub', 'ins']\n",
    "    \n",
    "# find the probability of the candidate in the corpus\n",
    "    for etype in edit_types:\n",
    "        for candidate in candidates[etype]:\n",
    "            if candidate in words:\n",
    "                candidatesProb[etype].append(all_words[candidate] / total_types)\n",
    "            else:\n",
    "                candidatesProb[etype].append(0)\n",
    "\n",
    "    \n",
    "# remove the candidates with 0 probability\n",
    "    for etype in edit_types:\n",
    "        i = 0    \n",
    "        while i != len(candidates[etype]):\n",
    "            if candidatesProb[etype][i] == 0:\n",
    "                del candidates[etype][i]\n",
    "                del candidatesProb[etype][i]\n",
    "                del candidates[etype + 'Edits'][i]\n",
    "                del candidates[etype + 'Correct'][i]\n",
    "            elif candidates[etype + 'Edits'][i] not in err_model:\n",
    "                del candidates[etype][i]\n",
    "                del candidatesProb[etype][i]\n",
    "                del candidates[etype + 'Edits'][i]\n",
    "                del candidates[etype + 'Correct'][i]\n",
    "            else:\n",
    "                p_of_c = candidatesProb[etype][i]\n",
    "                err_count = 0\n",
    "                for w in words:\n",
    "                    if candidates[etype + 'Correct'][i] in w:\n",
    "                        err_count += 1\n",
    "                p_of_w_given_c = err_model[candidates[etype + 'Edits'][i]] / err_count \n",
    "                temp.append([word, candidates[etype][i], etype, candidates[etype + 'Edits'][i], p_of_c, p_of_w_given_c, p_of_c * p_of_w_given_c])\n",
    "                i += 1\n",
    "                \n",
    "#     print(candidates)\n",
    "#     print(candidatesProb)\n",
    "#     print(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ddogs\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>candidate</th>\n",
       "      <th>edit_type</th>\n",
       "      <th>edit</th>\n",
       "      <th>P(c)</th>\n",
       "      <th>P(w|c)</th>\n",
       "      <th>P(c) x P(w|c)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddogs</td>\n",
       "      <td>dogs</td>\n",
       "      <td>del</td>\n",
       "      <td>&gt;d|&gt;</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7.929741e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word candidate edit_type  edit      P(c)    P(w|c)  P(c) x P(w|c)\n",
       "0  ddogs      dogs       del  >d|>  0.001228  0.000006   7.929741e-09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = input(\"Input: \")\n",
    "\n",
    "# call functions, return answer\n",
    "output = spellCorrect(word)\n",
    "print(\"Output: \")\n",
    "if len(output):\n",
    "    headers = ['word', 'candidate', 'edit_type', 'edit', 'P(c)', 'P(w|c)', 'P(c) x P(w|c)']\n",
    "    display(pandas.DataFrame(output, columns = headers))\n",
    "else:\n",
    "    print(\"No error\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Briefly describe the approach you took to implementing the spell correction assessment (both for detecting an error and correcting an error)\n",
    "- Did you make any modifications based from the method described in class?\n",
    "- What is an ideal corpus / edit distance matrix in relation to spell correction?\n",
    "- Do you think the corpus / edit distance matrix you used is good in relation to the task? Are there issues that you observed in using the data that might limit the performance of the model?\n",
    "- Are there ways we could modify the corpus / edit distance matrix to make the model more accommodating?\n",
    "- What can / can't your model do well? You may include examples.\n",
    "- Please note that you're not expected to create a perfect spell correction model, so there's no need to force the model to accommodate all known bugs of the model.\n",
    "- Do you think / can you imagine cases that the Noisy Channel cannot handle no matter what you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach that we took was to first check if the word was in our corpus so that we would know if the word has a spelling error or none. If there were to be a spelling error, then we would check for all of the possible edits within one edit distance and store the edit that happened. After this, we checked the candidates with our corpus to see if the words were part of the 'correct' words. It should be noted that since the corpus only consists of a number of words, it will not have all the words we currently use. After having the set of candidates, their edits, and probability of occuring in the corpus, we check the edits that were done to the single-edit spelling correction edits based off of Peter Norvig's collection of spelling errors. If there were edits that occured that did not appear in the collection, it would be removed from being a candidate. The remaining candidates would be used to show the possible correct spelling for the input while 'No error' would be the output if there were to be no error or candidates were not in the collection of spelling errors.\n",
    "\n",
    "We did a similar method described in class where we used the Damerau-Levenshtein Edit Distance to find the edits with distance 1, its type of correction and candidate word. With the language model for the candidate word, we were able to use it with the channel model from the edits and Peter Norvig's collection of spelling errors to produce the noisy channel model.\n",
    "\n",
    "An ideal corpus would be an English dictionary for English words because it would have the correct spelling of all the words and have a large number of words be in the corpus.\n",
    "\n",
    "The edit distance matrix used fits the task well since we were instructed to determine whether the word is a spelling error and all the words are 1-grams. With the scope limited to this, the edit distance matrix used where each operation costs one edit distance regardless covers a majority of the possible corrections.\n",
    "\n",
    "The corpus used in relation to the task is not good in relation to the task because it does not cover a majority of words, examples of these are contractions like can't or won't. \n",
    "\n",
    "When the data is missing in the error model and corpus it limits the performance of the model as what might be the suggested word might not exist in the given data. This may lead to wrong suggestions or correctly spelled words being flagged as spelling errors.\n",
    "\n",
    "The model can handle single error unigrams that are inside its corpus and error model well. This includes insertion, deletion, transposition, and substitution of letters and punctions as long as the word is within the corpus and the edit performed is inside the error model. For example, 'oze' would be corrected to 'ooze'.\n",
    "\n",
    "The model cannot handle words that are not in the corpus. For example, when we tried to input 'suceed' which should have an incorrect spelling because 'succeed' is the correct one, we saw that it could not see that it had a mistake. However, 'suceeded' had spelling corrections to 'succeeded'. Additionally, the model cannot handle errors not in the error model. For example, 'oze' would not have a suggestion of 'one' because there is no 'z|n' edit in it.\n",
    "\n",
    "Words that are not in different corpora yet that are real words, an example is when selfie was not yet declared a word it would have shown that it was a spelling error. As language is continually evolving new words will be created and until it is placed in the corpora the noisy channel model will not be able to handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "The noisy channel model is a model that is mainly used for spellcheckers and speech recognition. It uses data from an error model and a language model to form a probability of a word being suggested given an error detected within a given number of edit distances with the smaller edit distances made the higher the priority given. The noisy channel model is not perfect however as the higher the number of ngrams being taken into consideration the more noisy the channel becomes with more suggestions being given which not all will be helpful. It is also limited by the corpus and error model given to it and it views words not included in its dataset as non factors either to be not suggested or be flagged as a spelling error. Therefore, it can be said that by having a small ngram number in combination with a vast corpus and an error model, the noisy channel model can be a powerful model when it comes to checking the spelling of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4_Spell_Correction_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
