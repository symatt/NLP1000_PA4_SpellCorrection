{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Go, Wilfred | Sibug, Jordan | Sy, James Matthew\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/739602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\JAMES\n",
      "[nltk_data]     SY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "# load the words from the Gutenberg Document\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# print(\"Extracting all documents from NLTK's Project Gutenberg Collection...\")\n",
    "all_words = Counter()\n",
    "for filename in nltk.corpus.gutenberg.fileids():\n",
    "    words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
    "    all_words.update(words)\n",
    "#   print(\"%s; tokens: %d; vocab: %d\" % (filename, len(words), len(set(words))))\n",
    "\n",
    "# print(\"Overall Statistics\")\n",
    "# total_tokens = sum(all_words.values())\n",
    "total_types = len(all_words)\n",
    "\n",
    "# print(\"Total tokens: %d\" % total_tokens)\n",
    "# print(\"Total vocabulary / type: %d\" % total_types)\n",
    "# print(\"Type/token ratio: %.4f\" % (total_types / total_tokens))\n",
    "# print(\"Vocabulary richness: %.4f\" % (total_types / (total_tokens ** (1/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the word\n",
    "# print(set(words))\n",
    "# print(words[500])\n",
    "# print(words[100])\n",
    "# print(words[664])\n",
    "# print(words[32])\n",
    "# print(words[10000])\n",
    "# print(words[5])\n",
    "\n",
    "# get the count of the word\n",
    "# print(all_words[words[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Noisy Channel Model Implementation**\n",
    "\n",
    "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below. And while we discussed the general approach in class, kindly describe how you decided to implement the spell correction model. Include any modifications your group made as well. This might be a good spot to place part of your write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from http://norvig.com/spell-correct.html\n",
    "def edits1(word):\n",
    "    deletes = [] \n",
    "    deleteEdits = [] \n",
    "    transposes = [] \n",
    "    transposeEdits = []\n",
    "    replaces = [] \n",
    "    replaceEdits = []\n",
    "    inserts = []\n",
    "    insertEdits = []\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz-\\'\"\n",
    "#     [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "# DELETION\n",
    "# '' + 'te' = 'te' \n",
    "# if L[-1] == '': ('>' + R[0])|('>') == >h|>\n",
    "# 'h' + 'e' = 'he'  \n",
    "# (L[-1] + R[0])|(L[-1]) == ht|h\n",
    "# 'ht' + '' = 'ht'\n",
    "# (L[-1] + R[0])|(L[-1]) = te|t\n",
    "    for L,R in splits:\n",
    "        if R:\n",
    "            deletes.append(L + R[1:])\n",
    "            if not L: \n",
    "                deleteEdits.append('>' + str(R[0]) + '|>')\n",
    "            else:\n",
    "                 deleteEdits.append(str(L[-1] + R[0]) + '|' + str(L[-1]))\n",
    "\n",
    "# TRANSPOSITION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# ht|th R[0]+R[1] + '|' + R[1] + R[0]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    for L, R in splits:\n",
    "        if len(R) > 1:\n",
    "            transposes.append(L + R[1] + R[0] + R[2:])\n",
    "            transposeEdits.append(R[0]+R[1] + '|' + R[1] + R[0])\n",
    "# SUBSTITUTION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# e|i \n",
    "# R[0] + '|' + c\n",
    "# \n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            for c in letters:\n",
    "                replaces.append(L + c + R[1:])\n",
    "                replaceEdits.append(R[0] + '|' + c)\n",
    "    \n",
    "# INSERTION\n",
    "# [('', 'hte'), ('h', 'te'), ('ht', 'e'), ('hte', '')]\n",
    "# \n",
    "# >|>x if not L '>|>' + c\n",
    "# h|ha str(L[-1]) + '|' + str(L[-1]) + c\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    for L, R in splits:\n",
    "        for c in letters:\n",
    "            inserts.append(L + c + R)\n",
    "            if not L:\n",
    "                insertEdits.append('>|>' + c)\n",
    "            else:\n",
    "                insertEdits.append(str(L[-1]) + '|' + str(L[-1]) + c)\n",
    "    \n",
    "    # print(deleteEdits)\n",
    "    # print(insertEdits)\n",
    "    # print(replaceEdits)\n",
    "    # print(transposeEdits)\n",
    "    \n",
    "    wordsAndEdits = {\n",
    "        'del': deletes,\n",
    "        'tra': transposes,\n",
    "        'sub': replaces,\n",
    "        'ins': inserts,\n",
    "        'delEdits': deleteEdits,\n",
    "        'traEdits': transposeEdits,\n",
    "        'subEdits': replaceEdits,\n",
    "        'insEdits': insertEdits\n",
    "    }\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "    return wordsAndEdits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_err_model():\n",
    "    err_model = {}\n",
    "    err_count = 0\n",
    "\n",
    "    count_1edit = open('count_1edit.txt', 'r')\n",
    "\n",
    "    for line in count_1edit:\n",
    "        content = line.rstrip().split(\"\\t\")\n",
    "        key = str(content[0])\n",
    "        val = float(content[1])\n",
    "        err_count += val\n",
    "        err_model[key] = val\n",
    "\n",
    "    # print(err_model)\n",
    "\n",
    "    count_1edit.close()\n",
    "\n",
    "    return err_model, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCorrect(word):\n",
    "\n",
    "    err_model, err_count = load_err_model()\n",
    "    temp = []\n",
    "    # check if the word is in the corpus\n",
    "    if word in words:\n",
    "        return \"No error\"\n",
    "\n",
    "    # word should be corrected\n",
    "    # get possible words within edit distance 1\n",
    "    candidates = edits1(word)\n",
    "\n",
    "    candidatesProb = {\n",
    "        'del': [],\n",
    "        'tra': [],\n",
    "        'sub': [],\n",
    "        'ins': []\n",
    "    }\n",
    "#     print(wordsAndEdits)\n",
    "    edit_types = ['del', 'tra', 'sub', 'ins']\n",
    "    \n",
    "# find the probability of the candidate in the corpus\n",
    "    for etype in edit_types:\n",
    "        for candidate in candidates[etype]:\n",
    "            if candidate in words:\n",
    "                candidatesProb[etype].append(all_words[candidate] / total_types)\n",
    "            else:\n",
    "                candidatesProb[etype].append(0)\n",
    "\n",
    "    \n",
    "# remove the candidates with 0 probability\n",
    "    for etype in edit_types:\n",
    "        i = 0    \n",
    "        while i != len(candidates[etype]):\n",
    "            if candidatesProb[etype][i] == 0:\n",
    "                del candidates[etype][i]\n",
    "                del candidatesProb[etype][i]\n",
    "                del candidates[etype + 'Edits'][i]\n",
    "            elif candidates[etype + 'Edits'][i] not in err_model:\n",
    "                del candidates[etype][i]\n",
    "                del candidatesProb[etype][i]\n",
    "                del candidates[etype + 'Edits'][i]\n",
    "            else:\n",
    "                p_of_c = candidatesProb[etype][i]\n",
    "                p_of_w_given_c = err_model[candidates[etype + 'Edits'][i]] / err_count\n",
    "                temp.append([word, candidates[etype][i], etype, candidates[etype + 'Edits'][i], p_of_c, p_of_w_given_c, p_of_c * p_of_w_given_c])\n",
    "                i += 1\n",
    "                \n",
    "#     print(candidates)\n",
    "#     print(candidatesProb)\n",
    "#     print(temp)\n",
    "    if (len(temp)):\n",
    "        return temp\n",
    "    return \"No Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: the\n",
      "Output: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-52e7aa3b8c0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'candidate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'edit_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'edit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P(c)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P(w|c)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P(c) x P(w|c)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "word = input(\"Input: \")\n",
    "\n",
    "# call functions, return answer\n",
    "output = spellCorrect(word)\n",
    "print(\"Output: \")\n",
    "if len(output):\n",
    "    headers = ['word', 'candidate', 'edit_type', 'edit', 'P(c)', 'P(w|c)', 'P(c) x P(w|c)']\n",
    "    display(pandas.DataFrame(output, columns = headers))\n",
    "else:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Briefly describe the approach you took to implementing the spell correction assessment (both for detecting an error and correcting an error)\n",
    "- Did you make any modifications based from the method described in class?\n",
    "- What is an ideal corpus / edit distance matrix in relation to spell correction?\n",
    "- Do you think the corpus / edit distance matrix you used is good in relation to the task? Are there issues that you observed in using the data that might limit the performance of the model?\n",
    "- Are there ways we could modify the corpus / edit distance matrix to make the model more accommodating?\n",
    "- What can / can't your model do well? You may include examples.\n",
    "- Please note that you're not expected to create a perfect spell correction model, so there's no need to force the model to accommodate all known bugs of the model.\n",
    "- Do you think / can you imagine cases that the Noisy Channel cannot handle no matter what you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Briefly describe the approach you took to implementing the spell correction assessment (both for detecting an error and correcting an error)\n",
    "\n",
    "The approach that we took was to first check if the word was in our corpus so that we would know if the word has a spelling error or none. If there were to be a spelling error, then we would check for all of the possible edits within one edit distance and store the edit that happened. After this, we checked the candidates with our corpus to see if the words were part of the 'correct' words. It should be noted that since the corpus only consists of a number of words, it will not have all the words we currently use. After having the set of candidates, their edits, and probability of occuring in the corpus, we check the edits that were done to the single-edit spelling correction edits based off of Peter Norvig's collection of spelling errors. If there were edits that occured that did not appear in the collection, it would be removed from being a candidate. The remaining candidates would be used to show the possible correct spelling for the input while 'No error' would be the output if there were to be no error or candidates were not in the collection of spelling errors.\n",
    "\n",
    "- Did you make any modifications based from the method described in class?\n",
    "\n",
    "I think we did a similar method described in class where we used the Damerau-Levenshtein Edit Distance to find the edits with distance 1, its type of correction and candidate word. With the language model for the candidate word, we were able to use it with the channel model form the edits and Peter Norvig's collection of spelling errors to produce the noisy channel model.\n",
    "\n",
    "- What is an ideal corpus / edit distance matrix in relation to spell correction?\n",
    "\n",
    "An ideal corpus would be an English dictionary for English words because it would have the correct spelling of all the words and have a large number of words be in the corpus.\n",
    "\n",
    "- Do you think / can you imagine cases that the Noisy Channel cannot handle no matter what you do?\n",
    "\n",
    "Maybe words that are not in different corpora yet that are real words. Maybe before when selfie was not yet declared a word, it would have shown that it was a spelling error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4_Spell_Correction_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
